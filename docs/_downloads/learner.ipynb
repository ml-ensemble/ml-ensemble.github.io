{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n\n\n.. currentmodule:: mlens.parallel.learner\n\nLearner Mechanics\n=================\n\nML-Ensemble is designed to provide an easy user interface. But it is also designed\nto be extremely flexible, all the wile providing maximum concurrency at minimal\nmemory consumption. The lower-level API that builds the ensemble and manages the\ncomputations is constructed in as modular a fashion as possible.\n\nThe low-level API introduces a computational graph-like environment that you can\ndirectly exploit to gain further control over your ensemble. In fact, building\nyour ensemble through the low-level API is almost as straight forward as using the\nhigh-level API. In this tutorial, we will walk through the key core :class:`Learner` class.\n\n\nThe Learner API\n^^^^^^^^^^^^^^^\n\nBasics\n------\n\nWhen you pass an estimator to an ensemble, it gets wrapper\nin a :class:`Learner` instance. This class records relevant information\nabout the estimator and manages the cross-validated fit. It also keeps\ntrack of which preprocessing pipeline to use (if any). A learner is a parent node\nin a computational sub-graph induced by the cross-validation strategy.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mlens.utils.dummy import OLS\nfrom mlens.parallel import Learner\nfrom mlens.index import FoldIndex\n\n\nindexer = FoldIndex(folds=2)               # Define a training strategy\nlearner = Learner(estimator=OLS(),         # Declare estimator\n                  preprocess=None,         # We'll get to this\n                  indexer=indexer,         # Our above instance\n                  name='ols',              # Don't reuse name\n                  attr='predict',          # Attribute for prediction\n                  scorer=None,             # To get cv scores\n                  output_columns={0: 0},   # Prediction feature index\n                  verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. currentmodule:: mlens.index\n\nThe ``name`` gives the learner a cache reference. When the learner is\nconstructed by the high-level API , the name is guaranteed to be unique, but here\nyou must ensure all learner names are unique. The ``output_columns``\ntells the learner which column index in an output array it should populate\nwhen predicting. This helps us rapidly creating prediction with several learners. When\nwe have a unique prediction array use ``{0: 0}``. When the training strategy creates\npartitions, we need to map ``output_columns`` for each partition. We'll see an example of this below.\nThe ``attr`` argument tells the learner which method to use.\n\n.. currentmodule:: mlens.parallel.learner\n\nThe learner doesn't do any heavy lifting itself, it manages the creation a sub-graph\nof auxiliary :class:`SubLearner` nodes for each fold during estimation.\nThis process is dynamic: the sub-learners are temporary instance created for each\nestimation. To fit a learner, we first fit the indexer, then iterate through each of the\nsub-learners created for the task:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os, tempfile\nimport numpy as np\n\nX = np.arange(20).reshape(10, 2)\ny = np.random.rand(10)\n\n# Fit the indexer to data to create fold indexes\nindexer.fit(X)\n\n# Specify a cache directory\npath = tempfile.TemporaryDirectory(dir=os.getcwd())\n\n# Declare which type of job (fit, predict, transform) to run on which data\nfor sub_learner in learner.gen_fit(X, y):\n    sub_learner(path.name)\n\nprint(\"Cached items:\\n%r\" % os.listdir(path.name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fitting the learner puts three copies of the OLS estimator in the ``path``\ndirectory: one for each fold and one for the full dataset.\nThese are named as ``[name]__[col_id]__[fold_id]``. To load these into the\nlearner, call ``collect``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "learner.collect(path.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The main estimator, fitted on all data, gets stored into the\n``learner_`` attribute, while the others are stored in the\n``sublearners_``. These attributes are *generators* that create\nsub-learners on-the-fly from cached fitted estimators when called upon.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To generate predictions, we can either use the ``sublearners_``\ngenerator create cross-validated predictions, or ``learner_``\ngenerator to generate predictions for the whole input set.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similarly to above, we predict by specifying the job and the data to use.\nNote that now we also specify the output array to populate.\nIn particular, the learner will populate the columns given in the\n``output_columns`` parameter. Here, we use the ``transform`` task, which\nuses the ``sublearners_`` generator to produce cross-validated\npredictions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "P = np.zeros((y.shape[0], 1))\nfor sub_learner in learner.gen_transform(X, P):\n    sub_learner(path.name)\n    print('P:')\n    print(P)\n    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the above loop, a sub-segment of ``P`` is updated by each sublearner\nspawned by the learner. To instead produce predictions for the full\ndataset using the estimator fitted on all training data,\ntask the learner to ``predict``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "P = np.zeros((y.shape[0], 1))\nfor sub_learner in learner.gen_predict(X, P):\n    sub_learner(path.name)\n    print('P:')\n    print(P)\n    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ML-Ensemble follows the Scikit-learn API, so if you wish to update any\nhyper-parameters of the estimator, use the ``get_params`` and ``set_params``\nAPI:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Params before:\")\nprint(learner.get_params())\n\nlearner.set_params(estimator__offset=1, indexer__folds=3)\n\nprint(\"Params after:\")\nprint(learner.get_params())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>Updating the indexer on one learner updates the indexer on all</p></div>\n learners that where initiated with the same instance.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Partitioning\n------------\n\nWe can create several other types of learners by\nvarying the estimation strategy. An especially interesting strategy is to\npartition the training set and create several learners fitted on a given\npartition. This will create one prediction feature per partition.\nThe learner handles the computational graph for us, all we need to\ninput is a mapping between partitions and output columns in the\n``output_columns`` dict. In the following example we fit the OLS model\nusing two partitions and three fold CV on each\npartition. Note that by passing the output array as an argument during ``'fit'``,\nwe get predictions immediately.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mlens.index import SubsetIndex\n\ndef mse(y, p): return np.mean((y - p) ** 2)\n\nindexer = SubsetIndex(partitions=2, folds=2, X=X)\nlearner = Learner(estimator=OLS(),\n                  preprocess=None,\n                  indexer=indexer,\n                  name='ols',\n                  attr='predict',\n                  scorer=mse,\n                  output_columns={0: 0,    # First partition\n                                  1: 1},   # Second partition\n                  verbose=True)\n\n# P needs 2 cols\nP = np.zeros((y.shape[0], 2))\n\n# Pass P during 'fit' to get prediction immediately\nfor sub_learner in learner.gen_fit(X, y, P):\n    sub_learner.fit(path.name)\n    print('P:')\n    print(P)\n    print()\n\nlearner.collect(path.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each sub-learner records fit and predict times during fitting, and if\na scorer is passed scores the predictions as well. The learner aggregates\nthis data into a ``raw_data`` attribute in the form of a list.\nMore conveniently, the ``data`` attribute returns a dict with a specialized\nrepresentation that gives a tabular output directly:\nStandard data is fit time (``ft``), predict time (``pr``).\nIf a scorer was passed to the learner, cross-validated test set prediction\nscores are computed. For brevity, ``-m`` denotes the mean and ``-s``\ndenotes standard deviation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Data:\\n%s\" % learner.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preprocessing\n-------------\n\nWe can easily create a preprocessing pipeline before fitting the estimator.\nIn general, several estimators will share the same preprocessing pipeline,\nso we don't want to pipeline the transformations in the estimator itself\u2013\nthis will result in duplicate transformers.\nThe learner accepts a ``preprocess`` argument that points it to reference in\nthe estimation cache, and wil load the cached transformer for the given fold\nwhen running an estimation. This does mean that the input will be processed\nfor each estimator and each fold, but pre-processing the data and storing the\ndata does not scale as memory consumption grows exponentially.\nIn contrast, running (not fitting) a transformer pipeline is often an\nefficient operation that introduce only a minor overhead on computation time.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To facilitate preprocessing across several learners,\nwe need new type of node, the :class:`Transformer`. This class behaves\nsimilarly to the learner, but differs in that it doesn't output any\npredictions or transformations, but merely fits a pipeline and caches it\nfor the learner to load when needed. To construct a learner with\na preprocessing pipeline, we begin by constructing the\ntransformer.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mlens.utils.dummy import Scale\nfrom mlens.parallel import Transformer\n\ntransformer = Transformer(estimator=[('trans', Scale())],  # the pipeline should be a list of transformers\n                          indexer=indexer,\n                          name='sc',\n                          verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, to build the learner we now pass the ``name`` of the transformer as\nthe ``preprocess`` argument to the learner.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "learner = Learner(estimator=OLS(),\n                  preprocess='sc',\n                  indexer=indexer,\n                  name='ols',\n                  attr='predict',\n                  scorer=mse,\n                  output_columns={0: 0, 1: 1},\n                  verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now repeat the above process to fit the learner, starting with fitting\nthe transformer. Both follow the same API.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "P = np.zeros((y.shape[0], 2))\n\nfor st in transformer.gen_fit(X, y):\n    st(path.name)\n\nfor lr in learner.gen_fit(X, y, P):\n    lr(path.name)\n\ntransformer.collect(path.name)\nlearner.collect(path.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the cache now contains the transformers as well:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Cache: %r\" % os.listdir(path.name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data is collected on a partition basis:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Data:\\n%s\" % learner.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Parallel estimation\n-------------------\n\nSince the learner and transformer class do not perform estimations themselves,\nwe are free to modify the estimation behavior. For instance, to parallelize\nestimation with several learners, we don't want a nested loop over each learner,\nbut instead flatten the for loops for maximal concurrency.\nThis is the topic of our next walkthrough, here we show how to parallelize\nestimation with a single learner. Using the integrated :mod:`joblib` package, we can fit a\nlearner in parallel as follow:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mlens.externals.joblib import Parallel, delayed\nfrom numpy.testing import assert_array_equal\n\nP_t = np.zeros((y.shape[0], 2))\n\n# Since ML-Ensemble is thread-safe, we use threading as P_t is not memmapped.\nwith Parallel(backend='threading', n_jobs=-1) as parallel:\n    parallel(delayed(sublearner, check_pickle=False)(path.name)\n             for sublearner in learner.gen_fit(X, y, P_t)\n             )\n\nassert_array_equal(P, P_t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Joblib is built on top of the :mod:`multiprocessing` package, and we\ncan similarly directly use the ``Pool().map()`` API to achieve the same\nresult:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# The dummy module wraps the threading package in the multiprocessing API\nfrom multiprocessing.dummy import Pool\n\ndef run(tup): tup[0](tup[1])\n\nP_p = np.zeros((y.shape[0], 2))\nPool(4).map(run, [(sublearner, path.name)\n                  for sublearner in learner.gen_fit(X, y, P_p)])\n\nassert_array_equal(P, P_t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we handle several learners by grouping them in a layer in the\n`layer mechanics tutorial <layer_tutorial>`.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}