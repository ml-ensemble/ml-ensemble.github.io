{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n\n\n.. currentmodule: mlens.parallel\n\nParallel Mechanics\n==================\n\nML-Ensemble is designed to provide an easy user interface. But it is also designed\nto be extremely flexible, all the wile providing maximum concurrency at minimal\nmemory consumption. The lower-level API that builds the ensemble and manages the\ncomputations is constructed in as modular a fashion as possible.\n\nThe low-level API introduces a computational graph-like environment that you can\ndirectly exploit to gain further control over your ensemble. In fact, building\nyour ensemble through the low-level API is almost as straight forward as using the\nhigh-level API. In this tutorial, we will walk through the core\n:class:`ParallelProcessing` class.\n\nThe purpose of the :class:`ParallelProcessing` class is to provide a streamlined\ninterface for scheduling and allocating jobs in a nested sequence of tasks. The\ntypical case is a sequence of :class:`Layer` instances where the output of one layer\nbecomes the input to the next. While the layers must therefore be fitted sequentially,\neach layer should be fitted in parallel. We might be interested in propagating some of the\nfeatures from one layer to the next, in which case we need to take care of the array allocation.\n\nParallelProcessing API\n^^^^^^^^^^^^^^^^^^^^^^\n\nBasic map\n\u00a8\u00a8\u00a8\u00a8\u00a8\u00a8\u00a8\u00a8\u00a8\n\nIn the simplest case, we have a ``caller`` that has a set of ``task``s that needs to be\nevaluated in parallel. For instance, the ``caller`` might be a :class:`Learner`, with\neach task being a fit job for a given cross-validation fold. In this simple case,\nwe want to perform an embarrassingly parallel for-loop of each fold, which we can\nachieve with the ``map`` method of the :class:`ParallelProcessing` class.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mlens.parallel import ParallelProcessing, Job, Learner\nfrom mlens.index import FoldIndex\nfrom mlens.utils.dummy import OLS\n\nimport numpy as np\n\nnp.random.seed(2)\n\nX = np.arange(20).reshape(10, 2)\ny = np.random.rand(10)\n\nindexer = FoldIndex(folds=2)\nlearner = Learner(estimator=OLS(),\n                  indexer=indexer,\n                  name='ols')\n\nmanager = ParallelProcessing(n_jobs=-1)\n\nout = manager.map(learner, 'fit', X, y, return_preds=True)\n\nprint(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Stacking a set of parallel jobs\n-------------------------------\n\nSuppose instead that we have a sequence of learners, where we want to fit\neach on the errors of the previous learner. We can achieve this by using\n``stack`` method and a preprocessing pipeline for computing the errors.\nFirst, we need to construct a preprocessing class to transform the input,\nwhich will be the preceding learner's predictions, into errors.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mlens.parallel import Transformer, Pipeline\nfrom mlens.utils.dummy import Scale\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n\ndef error_scorer(p, y):\n    return np.abs(p - y)\n\n\nclass Error(BaseEstimator, TransformerMixin):\n\n    \"\"\"Transformer that computes the errors of a base learners\"\"\"\n\n    def __init__(self, scorer):\n        self.scorer = scorer\n\n    def fit(self, X, y):\n        return self\n\n    def transform(self, X, y):\n        return self.scorer(X, y), y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we construct a sequence of tasks to compute, where the output of one\ntask will be the input to the next. Hence, we want a sequence of the form\n``[learner, transformer, ..., learner]``:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tasks = []\nfor i in range(3):\n    if i != 0:\n        pipeline = Pipeline([('err', Error(error_scorer))], return_y=True)\n        transformer = Transformer(\n            estimator=pipeline,\n            indexer=indexer,\n            name='sc-%i' % (i + 1)\n        )\n        tasks.append(transformer)\n\n    learner = Learner(\n        estimator=OLS(),\n        preprocess='sc-%i' % (i+1) if i != 0 else None,\n        indexer=indexer,\n        name='ols-%i' % (i + 1)\n    )\n    tasks.append(learner)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To fit the stack, we call the ``stack`` method on the ``manager``, and since\neach learner must have access to their transformer, we set ``split=False``\n(otherwise each task will have a separate sub-cache, sealing them off\nfrom each other).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "out = manager.stack(\n    tasks, 'fit', X, y, return_preds=True, split=False)\n\nprint(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we instead want to append these errors as features, we can simply\nalter our transformer to concatenate the errors to the original data.\nAlternatively, we can automate the process by instead using the\n:class:`mlens.ensemble.Sequential`  API.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Manual initialization and processing\n------------------------------------\n\nUnder the hood, both ``map`` and ``stack`` first call ``initialize`` on the\n``manager``, followed by a call to ``process`` with some default arguments.\nFor maximum control, we can manually do the initialization and processing step.\nWhen we initialize, an instance of :class:`Job` is created that collect arguments\nrelevant for of the job as well as handles for data to be used. For instance,\nwe can specify that we want the predictions of all layers, as opposed to just the\nfinal layer:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "out = manager.initialize(\n    'fit', X, y, None, return_preds=['ols-1', 'ols-3'], stack=True, split=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ``initialize`` method primarily allocates memory of input data and\nputs it on the ``job`` instance. Not that if the input is a string pointing\nto data on disk, ``initialize`` will attempt to load the data into memory.\nIf the backend of the manger is ``threading``, keeping the data on the parent\nprocess is sufficient for workers to reach it. With ``multiprocessing`` as\nthe backend, data will be memory-mapped to avoid serialization.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ``initialize`` method returns an ``out`` dictionary that specified\nwhat type of output we want when running the manager on the assigned job.\nTo run the manager, we call ``process`` with out ``out`` pointer:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "out = manager.process(tasks, out)\nprint(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The output now is a list of arrays, the first contains the same predictions\nas we got in the ``map`` call, the last is the equivalent to the predicitons\nwe got in the ``stack`` call. Note that this functionality is available\nalso in the ``stack`` and ``map`` calls.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Memory management\n-----------------\n\nWhen running the manager, it will read and write to memory buffers. This is\nless of a concern when the ``threading`` backend is used, as data is kept\nin the parent process. But when data is loaded from file path, or when\n``multiprocessing`` is used, we want to clean up after us. Thus, when we\nare through with the ``manager``, it is important to call the ``clear``\nmethod. This will however destroy any ephemeral data stored on the instance.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "manager.clear()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "..warning:: The ``clear`` method will remove any files in the specified path.\nIf the path specified in the ``initialize`` call includes files other than\nthose generated in the ``process`` call, these will ALSO be removed.\nALWAYS use a clean temporary cache for processing jobs.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To minimize the risk of forgetting this last step, the :class:`ParallelProcessing`\nclass can be used as context manager, automatically cleaning up the cache\nwhen exiting the context:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "learner = Learner(estimator=OLS(), indexer=indexer)\n\nwith ParallelProcessing() as mananger:\n    manager.stack(learner, 'fit', X, y, split=False)\n    out = manager.stack(learner, 'predict', X, split=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}