{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n\n\n.. currentmodule:: mlens.parallel\n\nLearner Mechanics\n=================\n\nML-Ensemble is designed to provide an easy user interface. But it is also designed\nto be extremely flexible, all the wile providing maximum concurrency at minimal\nmemory consumption. The lower-level API that builds the ensemble and manages the\ncomputations is constructed in as modular a fashion as possible.\n\nThe low-level API introduces a computational graph-like environment that you can\ndirectly exploit to gain further control over your ensemble. In fact, building\nyour ensemble through the low-level API is almost as straight forward as using the\nhigh-level API. In this tutorial, we will walk through the basics :class:`Learner`\nand :class:`Transformer` class.\n\n\nThe Learner API\n^^^^^^^^^^^^^^^\n\nBasics\n------\n\nThe base estimator of ML-Ensemble is the :class:`Learner` instance. A learner is a\nwrapper around a generic estimator along with a cross-validation strategy. The job\nof the learner is to manage all sub-computations required for fitting and prediction.\nIn fact, it's public methods are generators from sub-learners, that do the actual\ncomputation.  A learner is the parent node of an estimator's computational sub-graph\ninduced by the cross-validation strategy.\n\nA learner is created by specifying an ``estimator`` and an ``indexer``, along with a\nset of optional arguments, most notably the ``name`` of the learner. Naming is important,\nis it is used for cache referencing. If setting it manually, ensure you give the learner\na unique name.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mlens.utils.dummy import OLS\nfrom mlens.parallel import Learner, Job\nfrom mlens.index import FoldIndex\n\n\nindexer = FoldIndex(folds=2)\nlearner = Learner(estimator=OLS(),\n                  indexer=indexer,\n                  name='ols')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The learner doesn't do any heavy lifting itself, it manages the creation a sub-graph\nof auxiliary :class:`SubLearner` nodes for each fold during estimation.\nThis process is dynamic: the sub-learners are temporary instances created for each\nestimation.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To fit a learner, we need a cache reference. When fitting all estimators from the\nmain process, this reference can be a list. If not (e.g. multiprocessing), the\nreference should instead be a ``str`` pointing to the path of the cache directory.\nPrior to running a job (``fit``, ``predict``, ``transform``), the learner must be\nconfigured on the given data by calling the ``setup`` method. This takes cares of\nindexing the training set for cross-validation, assigning output columns et.c.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os, tempfile\nimport numpy as np\n\nX = np.arange(20).reshape(10, 2)\ny = np.random.rand(10)\n\n# Specify a cache directory\npath = []\n\n# Run the setup routine\nlearner.setup(X, y, 'fit')\n\n# Run\nfor sub_learner in learner.gen_fit(X, y):\n    sub_learner.fit(path)\n\nprint(\"Cached items:\\n%r\" % path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fitting the learner puts three copies of the OLS estimator in the ``path``:\none for each fold and one for the full dataset.\nThese are named as ``[name].[col_id].[fold_id]``. To load these into the\nlearner, we need to call ``collect``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "learner.collect(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The main estimator, fitted on all data, gets stored into the\n``learner_`` attribute, while the others are stored in the\n``sublearners_``. These attributes are generators that create\nnew sub-learners with fitted estimators when called upon.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To generate predictions, we can either use the ``sublearners_``\ngenerator create cross-validated predictions, or ``learner_``\ngenerator to generate predictions for the whole input set.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similarly to above, we predict by specifying the job and the data to use.\nNow however, we must also specify the output array to populate.\nIn particular, the learner will populate the columns given in the\n``output_columns`` attribute, which is set with the ``setup`` call. If you\ndon't want it to start populating from the first column, you can pass the\n``n_left_concats`` argument to ``setup``. Here, we use the ``transform`` task,\nwhich uses the ``sublearners_`` generator to produce cross-validated\npredictions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "path = []\nP = np.zeros((y.shape[0], 2))\nlearner.setup(X, y, 'transform', n_left_concats=1)\nfor sub_learner in learner.gen_transform(X, P):\n    sub_learner.transform(path)\n    print('Output:')\n    print(P)\n    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the above loop, a sub-segment of ``P`` is updated by each sublearner\nspawned by the learner. To instead produce predictions for the full\ndataset using the estimator fitted on all training data,\ntask the learner to ``predict``.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To streamline job generation across tasks and different classes, ML-Ensemble\nfeatures a :class:`Job` class that manages job parameters.\nThe job class prevents code repetition and allows us to treat the learner\nas a callable, enabling task-agnostic code:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "job = Job(\n    job='predict',\n    stack=False,\n    split=True,\n    dir={},\n    targets=y,\n    predict_in=X,\n    predict_out=np.zeros((y.shape[0], 1))\n)\n\nlearner.setup(job.predict_in, job.targets, job.job)\nfor sub_learner in learner(job.args(), 'main'):\n    sub_learner()\n    print('Output:')\n    print(job.predict_out)\n    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ML-Ensemble follows the Scikit-learn API, so if you wish to update any\nhyper-parameters of the estimator, use the ``get_params`` and ``set_params``\nAPI:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Params before:\")\nprint(learner.get_params())\n\nlearner.set_params(estimator__offset=1, indexer__folds=3)\n\nprint(\"Params after:\")\nprint(learner.get_params())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>Updating the indexer on one learner updates the indexer on all</p></div>\n learners that where initiated with the same instance.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Partitioning\n------------\n\nWe can create several other types of learners by\nvarying the estimation strategy. An especially interesting strategy is to\npartition the training set and create several learners fitted on a given\npartition. This will create one prediction feature per partition.\nIn the following example we fit the OLS model using two partitions and\nthree fold CV on each partition. Note that by passing the output array\nas an argument during ``'fit'``, we perform a fit and transform operation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mlens.index import SubsetIndex\n\ndef mse(y, p): return np.mean((y - p) ** 2)\n\nindexer = SubsetIndex(partitions=2, folds=2, X=X)\nlearner = Learner(estimator=OLS(),\n                  indexer=indexer,\n                  name='subsemble-ols',\n                  scorer=mse,\n                  verbose=True)\n\njob.job = 'fit'\njob.predict_out = np.zeros((y.shape[0], 2))\n\nlearner.setup(job.predict_in, job.targets, job.job)\nfor sub_learner in learner(job.args(), 'main'):\n    sub_learner.fit()\n    print('Output:')\n    print(job.predict_out)\n    print()\n\nlearner.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each sub-learner records fit and predict times during fitting, and if\na scorer is passed scores the predictions as well. The learner aggregates\nthis data into a ``raw_data`` attribute in the form of a list.\nMore conveniently, the ``data`` attribute returns a dict with a specialized\nrepresentation that gives a tabular output directly:\nStandard data is fit time (``ft``), predict time (``pr``).\nIf a scorer was passed to the learner, cross-validated test set prediction\nscores are computed. For brevity, ``-m`` denotes the mean and ``-s``\ndenotes standard deviation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Data:\\n%s\" % learner.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preprocessing\n-------------\n\nWe can easily create a preprocessing pipeline before fitting the estimator.\nIn general, several estimators will share the same preprocessing pipeline,\nso we don't want to pipeline the transformations in the estimator itself\u2013\nthis will result in duplicate transformers.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As with estimators, transformers too define a computational sub-graph given\na cross-validation strategy. Preprocessing pipelines are therefore wrapped\nby the :class:`Transformer` class, which is similar to the :class:`Learner`\nclass. The input to the Transformer is a :class:`Pipeline` instance that holds the\npreprocessing pipeline.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>When constructing a :class:`Pipeline` for use with the :class:`Transformer`,\n  the ``return_y`` argument must be ``True``.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To link the transformer's sub-graph with the learner's sub-graph,\nwe set the ``preprocess`` argument of the learner equal to the ``name``\nof the :class:`Transformer`. Note that any number of learners can share\nthe same transformer and in fact should when the same preprocessing is desired.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mlens.utils.dummy import Scale\nfrom mlens.parallel import Transformer, Pipeline\n\npipeline = Pipeline([('trans', Scale())], return_y=True)\n\ntransformer = Transformer(estimator=pipeline,\n                          indexer=indexer,\n                          name='sc',\n                          verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To build the learner we pass the ``name`` of the transformer as\nthe ``preprocess`` argument:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "learner = Learner(estimator=OLS(),\n                  preprocess='sc',\n                  indexer=indexer,\n                  scorer=mse,\n                  verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now repeat the above process to fit the learner, starting with fitting\nthe transformer. By using the :class:`Job` class, we can write task-agnostic\nboiler-plate code. Note that the transformer is called as an\n``'auxiliary'`` task, while the learner is called as the ``'main'`` task.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Reset the prediction output array\njob.predict_out = np.zeros((y.shape[0], 2))\n\ntransformer.setup(job.predict_in, job.targets, job.job)\nlearner.setup(job.predict_in, job.targets, job.job)\n\n# Turn split off when you don't want the args() call to spawn a new sub-cache\njob.split = False\nfor subtransformer in transformer(job.args(), 'auxiliary'):\n    subtransformer()\n\nfor sublearner in learner(job.args(), 'main'):\n    sublearner()\n\ntransformer.collect()\nlearner.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the cache now contains the transformers as well:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Cache:\")\nfor item in job.dir['task_%i' % job._n_dir]:\n    print('{:20}{}'.format(*item))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And estimation data is collected on a partition basis:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Data:\\n%s\" % learner.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Parallel estimation\n-------------------\n\nSince the learner and transformer class do not perform estimations themselves,\nwe are free to modify the estimation behavior. For instance, to parallelize\nestimation with several learners, we don't want a nested loop over each learner,\nbut instead flatten the for loops for maximal concurrency.\nThis is the topic of our next walk through. Here we show how to parallelize\nestimation with a single learner using multiple threads:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from multiprocessing.dummy import Pool\n\ndef run(est): est()\n\nargs = job.args()\njob.predict_out = np.zeros((y.shape[0], 2))\njob.job = 'predict'\nPool(4).map(run, list(learner(args, 'main')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For a slightly more high-level API for parallel computation on a single\ninstance (of any accepted class), we can turn to the :func:`run` function.\nThis function takes care of argument specification, array creation and all\ndetails we would otherwise need to attend to. For instance, to transform\na dataset using the preprocessing pipeline fitted on the full training set,\nuse :func:`run` to call ``predict``:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mlens.parallel import run\n\nprint(\n    run(transformer, 'predict', X)\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we handle several learners by grouping them in a layer in the\n`layer mechanics tutorial <layer_tutorial>`.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}