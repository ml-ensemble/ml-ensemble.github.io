{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n\n.. currentmodule:: mlens.parallel\n\nLayer Mechanics\n===============\n\nML-Ensemble is designed to provide an easy user interface. But it is also designed\nto be extremely flexible, all the wile providing maximum concurrency at minimal\nmemory consumption. The lower-level API that builds the ensemble and manages the\ncomputations is constructed in as modular a fashion as possible.\n\nThe low-level API introduces a computational graph-like environment that you can\ndirectly exploit to gain further control over your ensemble. In fact, building\nyour ensemble through the low-level API is almost as straight forward as using\nthe high-level API. In this tutorial, we will walk through how to use the\n:class:`Group` and :class:`Layer` classes to fit several learners.\n\nSuppose we want to fit several learners. The ` learner tutorial <learner_tutorial`\nshowed us how to fit a single learner, and so one approach would be to simple\niterate over our learners and fit them one at a time. This however is a very slow\napproach since we don't exploit the fact that learners can be trained in parallel.\nMoreover, any type of aggregation, like putting all predictions into an array, would\nhave to be done manually.\n\nThe Layer API\n^^^^^^^^^^^^^\n\nTo parallelize the implementation, we can use the :class:`Layer` class. A layer is\na handle that will run any number of :class:`Group`s attached to it in parallel. Each\ngroup in turn is a wrapper around a ``indexer-transformers-estimators`` triplet.\n\nBasics\n------\n\nSo, to fit our two learners in parallel, we first need a :class:`Group` object to\nhandle them.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mlens.parallel import Layer, Group, make_group, run\nfrom mlens.utils.dummy import OLS, Scale\nfrom mlens.index import FoldIndex\n\n\nindexer = FoldIndex(folds=2)\ngroup = make_group(indexer, [OLS(1), OLS(2)], None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This ``group`` object is now a complete description of how to fit our two\nlearners using the prescribed indexing method.\n\nTo train the estimators, we need feed the group to a :class:`Layer` instance:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nnp.random.seed(2)\n\nX = np.arange(20).reshape(10, 2)\ny = np.random.rand(10)\n\n\n\nlayer = Layer(stack=group)\n\nprint(\n    run(layer, 'fit', X, y, return_preds=True)\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To use some preprocessing before fitting the estimators, we can use the\n``transformers`` argument when creating our ``group``:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "group = make_group(indexer, [OLS(1), OLS(2)], [Scale()])\n\nlayer = Layer(stack=group)\n\nprint(\n    run(layer, 'fit', X, y, return_preds=True)\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Multitasking\n------------\n\nIf we want our estimators two have different preprocessing, we can easily\nachieve this either by specifying different cases when making the group,\nor by making two separate groups. In the first case:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "group = make_group(\n    indexer,\n    {'case-1': [OLS(1)], 'case-2': [OLS(2)]},\n    {'case-1': [Scale()], 'case-2': []}\n)\n\nlayer = Layer(stack=group)\n\nprint(\n    run(layer, 'fit', X, y, return_preds=True)\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the latter case:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "groups = [\n    make_group(indexer, OLS(1), Scale()), make_group(indexer, OLS(2), None)\n]\n\nlayer = Layer(stack=groups)\n\nprint(\n    run(layer, 'fit', X, y, return_preds=True)\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Which method to prefer depends on the application, but generally, it is\npreferable to put all transformers and all estimators belonging to a\ngiven indexing strategy into one ``group`` instance as it is easier to\nseparate groups based on indexer and using cases to distinguish between\ndifferent preprocessing pipelines.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, suppose we want to do something more exotic, like using different\nindexing strategies for different estimators. This can easily be achieved\nby creating groups for each indexing strategy we want:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "groups = [\n    make_group(FoldIndex(2), OLS(1), Scale()),\n    make_group(FoldIndex(4), OLS(2), None)\n]\n\nlayer = Layer(stack=groups)\n\nprint(\n    run(layer, 'fit', X, y, return_preds=True)\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Some care needs to be taken here: if indexing strategies do not return the\nsame number of rows, the output array will be zero-padded.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mlens.index import BlendIndex\n\ngroups = [\n    make_group(FoldIndex(2), OLS(1), None),\n    make_group(BlendIndex(0.5), OLS(1), None)\n]\n\nlayer = Layer(stack=groups)\nprint(\n    run(layer, 'fit', X, y, return_preds=True)\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that even if ``mlens`` indexer output different shapes, they preserve\nrow indexing to ensure predictions are consistently mapped to their respective\ninput. If you build a custom indexer, make sure that it uses a strictly\nsequential (with respect to row indexing) partitioning strategy.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Layer features\n--------------\n\nA layer does not have to be specified all in one go; you can instantiate\na layer and ``push`` and ``pop`` to its ``stack``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "layer = Layer()\ngroup = make_group(FoldIndex(4), OLS(), None)\nlayer.push(group)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p></p></div>\n\nIf you push or pop to the stack, you must call ``fit`` before you can\nuse the layer for prediction.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "run(layer, 'fit', X, y)\n\ngroup = make_group(FoldIndex(2), OLS(1), None)\nlayer.push(group)\n\ntry:\n    run(layer, 'predict', X, y)\nexcept Exception as exc:\n    print(\"Error: %s\" % str(exc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The :class:`Layer` class can print the progress of a job, as well as inspect\ndata collected during the job. Note that the\nprintouts of the layer does not take group membership into account.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mlens.metrics import rmse\n\nlayer = Layer()\ngroup1 = make_group(\n    indexer,\n    {'case-1': [OLS(1)], 'case-2': [OLS(2)]},\n    {'case-1': [Scale()], 'case-2': []},\n    learner_kwargs={'scorer': rmse}\n)\n\nlayer.push(group1)\n\nrun(layer, 'fit', X, y, return_preds=True)\nprint()\nprint(\"Collected data:\")\nprint(layer.data)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}