{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\nAdvanced features tutorial\n==========================\n\nThe following tutorials highlight advanced functionality and provide in-depth\nmaterial on ensemble APIs.\n\n===============================  ==============================================\nTutorial                         Content\n===============================  ==============================================\n`propa-tutorial`            Propagate feature input features through layers\n\\                                to allow several layers to see the same input.\n`proba-tutorial`            Build layers that output class probabilities from each base\n\\                                learner so that the next layer or meta estimator learns\n\\                                from probability distributions.\n`subsemble-tutorial`        Learn homogenous partitions of feature space\n\\                                that maximize base learner's performance on each partition.\n`sequential-tutorial`       How to build ensembles with different layer classes\n`memory-tutorial`           Avoid loading data into the parent process by specifying a\n\\                                file path to a memmaped array or a csv file.\n`model-selection-tutorial`  Build transformers that replicate layers in ensembles for\n\\                                model selection of higher-order layers and / or meta learners.\n===============================  ==============================================\n\nWe use the same preliminary settings as in the\n`getting started <getting-started>` section.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom pandas import DataFrame\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.datasets import load_iris\n\nseed = 2017\nnp.random.seed(seed)\n\ndata = load_iris()\nidx = np.random.permutation(150)\nX = data.data[idx]\ny = data.target[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nPropagating input features\n--------------------------\n\nWhen stacking several layers of base learners, the variance of the input\nwill typically get smaller as learners get better and better at predicting\nthe output and the remaining errors become increasingly difficult to correct\nfor. This multicolinearity can significantly limit the ability of the\nensemble to improve upon the best score of the subsequent layer as there is too\nlittle variation in predictions for the ensemble to learn useful combinations.\nOne way to increase this variation is to propagate features from the original\ninput and / or earlier layers. To achieve this in ML-Ensemble, we use the\n``propagate_features`` attribute. To see how this works, let's compare\na three-layer ensemble with and without feature propagation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mlens.ensemble import SuperLearner\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\n\ndef build_ensemble(incl_meta, propagate_features=None):\n    \"\"\"Return an ensemble.\"\"\"\n    if propagate_features:\n        n = len(propagate_features)\n        propagate_features_1 = propagate_features\n        propagate_features_2 = [i for i in range(n)]\n    else:\n        propagate_features_1 = propagate_features_2 = None\n\n    estimators = [RandomForestClassifier(random_state=seed), SVC()]\n\n    ensemble = SuperLearner()\n    ensemble.add(estimators, propagate_features=propagate_features_1)\n    ensemble.add(estimators, propagate_features=propagate_features_2)\n\n    if incl_meta:\n        ensemble.add_meta(LogisticRegression())\n    return ensemble"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Without feature propagation, the meta learner will learn from the predictions\nof the penultimate layers:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "base = build_ensemble(False)\nbase.fit(X, y)\npred = base.predict(X)[:5]\nprint(\"Input to meta learner :\\n %r\" % pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When we propagate features, some (or all) of the input seen by one layer is\npassed along to the next layer. For instance, we can propagate some or all of\nthe input array through our two intermediate layers to the meta learner input\nof the meta learner:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "base = build_ensemble(False, [1, 3])\nbase.fit(X, y)\npred = base.predict(X)[:5]\nprint(\"Input to meta learner :\\n %r\" % pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this scenario, the meta learner will see noth the predictions made by the\npenultimate layer, as well as the second and fourth feature of the original\ninput. By propagating\nfeatures, the issue of multicolinearity in deep ensembles can be mitigated.\nIn particular, it can give the meta learner greater opportunity to identify\nneighborhoods in the original feature space where base learners struggle. We\ncan get an idea of how feature propagation works with our toy example. First,\nwe need a simple ensemble evaluation routine. In our case, propagating the\noriginal features through two layers of the same\nlibrary of base learners gives a dramatic increase in performance on the test\nset:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def evaluate_ensemble(propagate_features):\n    \"\"\"Wrapper for ensemble evaluation.\"\"\"\n    ens = build_ensemble(True, propagate_features)\n    ens.fit(X[:75], y[:75])\n    pred = ens.predict(X[75:])\n    return accuracy_score(pred, y[75:])\n\n\nscore_no_prep = evaluate_ensemble(None)\nscore_prep = evaluate_ensemble([0, 1, 2, 3])\nprint(\"Test set score no feature propagation  : %.3f\" % score_no_prep)\nprint(\"Test set score with feature propagation: %.3f\" % score_prep)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# .. py:currentmodule:: mlens.preprocessing\n#\n# By combining feature propagation with the :class:`Subset` transformer, you can\n# propagate the feature through several layers without any of the base estimators\n# in those layers seeing the propagated features. This can be desirable if you\n# want to propagate the input features to the meta learner without intermediate\n# base learners always having access to the original input data. In this case,\n# we specify propagation as above, but add a preprocessing pipeline to\n# intermediate layers:\n\nfrom mlens.preprocessing import Subset\n\nestimators = [RandomForestClassifier(random_state=seed), SVC()]\nensemble = SuperLearner()\n\n# Initial layer, propagate as before\nensemble.add(estimators, propagate_features=[0, 1])\n\n# Intermediate layer, keep propagating, but add a preprocessing\n# pipeline that selects a subset of the input\nensemble.add(estimators,\n             preprocessing=[Subset([2, 3])],\n             propagate_features=[0, 1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the above example, the two first features of the original input data\nwill be propagated through both layers, but the second layer will not be\ntrained on it. Instead, it will only see the predictions made by the base\nlearners in the first layer.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ensemble.fit(X, y)\nn = list(ensemble.layer_2.learners[0].learner)[0].estimator.feature_importances_.shape[0]\nm = ensemble.predict(X).shape[1]\nprint(\"Num features seen by estimators in intermediate layer: %i\" % n)\nprint(\"Num features in the output array of the intermediate layer: %i\" % m)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nProbabilistic ensemble learning\n-------------------------------\n\nWhen the target to predict is a class label, it can often be beneficial to\nlet higher-order layers or the meta learner learn from *class probabilities*,\nas opposed to the predicted class. Scikit-learn classifiers can return a\nmatrix that, for each observation in the test set, gives the probability that\nthe observation belongs to the a given class. While we are ultimately\ninterested in class membership, this information is much richer that just\nfeeding the predicted class to the meta learner. In essence, using class\nprobabilities allow the meta learner to weigh in not just the predicted\nclass label (the highest probability), but also with what confidence each\nestimator makes the prediction, and how estimators consider the alternative.\nFirst, let us set a benchmark ensemble performance when learning is by\npredicted class membership.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mlens.ensemble import BlendEnsemble\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\ndef build_ensemble(proba, **kwargs):\n    \"\"\"Return an ensemble.\"\"\"\n    estimators = [RandomForestClassifier(random_state=seed),\n                  SVC(probability=proba)]\n\n    ensemble = BlendEnsemble(**kwargs)\n    ensemble.add(estimators, proba=proba)   # Specify 'proba' here\n    ensemble.add_meta(LogisticRegression())\n\n    return ensemble"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As in the `ensemble guide <ensemble-guide>`, we fit on the first half,\nand test on the remainder.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ensemble = build_ensemble(proba=False)\nensemble.fit(X[:75], y[:75])\npreds = ensemble.predict(X[75:])\nprint(\"Accuracy:\\n%r\" % accuracy_score(preds, y[75:]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, to enable probabilistic learning, we set ``proba=True`` in the ``add``\nmethod for all layers except the final meta learner layer.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ensemble = build_ensemble(proba=True)\nensemble.fit(X[:75], y[:75])\npreds = ensemble.predict(X[75:])\nprint(\"\\nAccuracy:\\n%r\" % accuracy_score(preds, y[75:]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this case, using probabilities has a drastic effect on predictive\nperformance, increasing some 40 percentage points. For an applied example\nsee the ensemble used to beat the Scikit-learn `mnist` benchmark.\n\n\nAdvanced Subsemble techniques\n-----------------------------\n\n.. currentmodule:: mlens.ensemble\n\nSubsembles leverages the idea that neighborhoods of feature space have a\nspecific local structure. When we fit an estimator across all feature space,\nit is very hard to capture several such local properties. Subsembles partition\nthe feature space and fits each base learner to each partitions, thereby\nallow base learners to optimize locally. Instead, the task of generalizing\nacross neighborhoods is left to the meta learner. This strategy can be very\npowerful when the local structure first needs to be extracted, before an\nestimator can learn to generalize. Suppose you want to learn the probability\ndistribution of some variable $y$. Often, the true distribution is\nmulti-modal, which is an extremely hard problem. In fact, most\nmachine learning algorithms, especially with convex optimization objectives, are\nill equipped to solve this problem. Subsembles can overcome this issue allowing\nbase estimators to fit one mode of the distribution at a time, which yields a\nbetter representation of the distribution and greatly facilitates the learning\nproblem of the meta learner.\n\n.. py:currentmodule:: mlens.ensemble\n\nBy default, the :class:`Subsemble` class partitioning the dataset randomly.\nNote however that partitions are created on the data \"as is\", so if the ordering\nof observations is not random, neither will the partitioning be. For this\nreason, it is recommended to shuffle the data (e.g. via the ``shuffle``\noption at initialization). To build a subsemble with random partitions, the\nonly parameter needed is the number of ``partitions`` when instantiating\nthe :class:`Subsemble`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mlens.ensemble import Subsemble\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\n\ndef build_subsemble():\n    \"\"\"Build a subsemble with random partitions\"\"\"\n    sub = Subsemble(partitions=3, folds=2)\n    sub.add([SVC(), LogisticRegression()])\n    return sub\n\nsub= build_subsemble()\nsub.fit(X, y)\ns = sub.predict(X[:10]).shape[1]\nprint(\"No. prediction features: %i \" % s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "During training, the base learners are copied to each partition,\nso the output of each layer gets multiplied by the number of partitions. In this\ncase, we have 2 base learners for 3 partitions, giving 6 prediction features.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By creating partitions, subsembles `scale significantly better <bench>`\nthan the\n:class:`SuperLearner`, but in contrast to :class:`BlendEnsemble`,\nthe full training data is leveraged during training. But randomly partitioning\nthe data does however not exploit the full advantage of locality, since it is\nonly by luck that we happen to create such partitions. A better way is to\n*learn* how to best partition the data. We can either use\nunsupervised algorithms to generate clusters, or supervised estimators and\ncreate partitions based on their predictions. In ML-Ensemble, this is\nachieved by passing an estimator as ``partition_estimator``. This estimator\ncan differ between layers.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. currentmodule:: mlens.index\n\nVery few limitation are imposed on the estimator: you can specify whether\nyou want to fit it before generating partitions, whether to use\nlabels in the partitioning, and what method to call to generate the\npartitions. See :class:`ClusteredSubsetIndex` for the full documentation.\nThis level of generality does impose some\nresponsibility on the user. In particular, it is up to the user to ensure that\nsensible partitions are created. Problems to watch out for is too small\npartitions (too many clusters, too uneven cluster sizes) and clusters with too\nlittle variation: for instance with only a single class label in the entire\npartition, base learners have nothing to learn. Let's see how to do this in\npractice. For instance, we can use an unsupervised K-Means\nclustering estimator to partition the data, like so:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n\ndef build_clustered_subsemble(estimator):\n    \"\"\"Build a subsemble with random partitions\"\"\"\n    sub = Subsemble(partitions=2,\n                    partition_estimator=estimator,\n                    folds=2, verbose=2)\n\n    sub.add([SVC(), LogisticRegression()])\n    sub.add_meta(SVC())\n    return sub\n\nsub = build_clustered_subsemble(KMeans(2))\nsub.fit(X[:, [0, 1]], y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Iris dataset can actually separate the classes perfectly with a KMeans\nestimator which leads to zero label variation in each partition. For that\nreason the above code fits the KMeans estimator on only the first two\ncolumns. But this approach is nota very good way of doing it since we loose\nthe rest of the data when fitting the estimators too. Instead, we could\ncustomize the\npartitioning estimator to make the subset selection itself. For instance,\nwe can use Scikit-learn's :class:`sklearn.pipeline.Pipeline`\nclass to put a dimensionality reduction transformer before the partitioning\nestimator, such as a :class:`sklearn.decomposition.PCA`, or the\n:class:`mlens.preprocessing.Subset` transformer to drop some features before\nestimation. We then use this pipeline as a our partition estimator and fit\nthe subsemble on all features.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mlens.preprocessing import Subset\nfrom sklearn.pipeline import make_pipeline\n\n# This partition estimator is equivalent to the one used above\npe = make_pipeline(Subset([0, 1]), KMeans(2))\nsub = build_clustered_subsemble(pe)\n\nsub.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In general, you may need to wrap an estimator around a custom class to modify\nit's output to generate good partitions. For instance, in regression problems,\nthe output of a supervised estimator needs to be binarized to give a discrete\nnumber of partitions. Here's minimalist way of wrapping a Scikit-learn\nestimator:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n\nclass MyClass(LinearRegression):\n\n    def __init__(self, **kwargs):\n        super(MyClass, self).__init__(**kwargs)\n\n    def fit(self, X, y):\n        \"\"\"Fit estimator.\"\"\"\n        super(MyClass, self).fit(X, y)\n        return self\n\n    def predict(self, X):\n        \"\"\"Generate partition\"\"\"\n        p = super(MyClass, self).predict(X)\n        return 1 * (p > p.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, to summarize the functionality in one example,\nlet's implement a simple (but rather useless) partition estimator that splits\nthe data in half based on the sum of the features.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class SimplePartitioner():\n\n    def __init__(self):\n        pass\n\n    def our_custom_function(self, X, y=None):\n        \"\"\"Split the data in half based on the sum of features\"\"\"\n        # Labels should be numerical\n        return 1 * (X.sum(axis=1) > X.sum(axis=1).mean())\n\n# Note that the number of partitions the estimator creates *must* match the\n# ``partitions`` argument passed to the Subsemble.\n# The ``folds`` option is completely independent.\n\nsub = Subsemble(partitions=2, folds=3, verbose=1)\nsub.add([SVC(), LogisticRegression()],\n        partition_estimator=SimplePartitioner(),\n        fit_estimator=False,\n        attr=\"our_custom_function\")\n\nsub.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A final word of caution. When implementing custom estimators from scratch, some\ncare needs to be taken if you plan on copying the Subsemble. It is advised that\nthe estimator inherits the :class:`sklearn.base.BaseEstimator` class to\nprovide a Scikit-learn compatible interface. For further information,\nsee the `API` documentation of the :class:`Subsemble`\nand :class:`mlens.base.indexer.ClusteredSubsetIndex`.\n\nFor an example of using clustered subsemble, see the subsemble\nused to beat the Scikit-learn `mnist` benchmark.\n\n\nGeneral multi-layer ensemble learning\n-------------------------------------\n\n.. currentmodule:: mlens.ensemble\n\nThe modular ``add`` API of ML-Ensembles allow users to build arbitrarily\ndeep ensembles. If you would like to alternate between the *type* of each layer\nthe :class:`SequentialEnsemble` class can be used to specify what type of\nlayer (i.e. stacked, blended, subsamle-style) to add. This can be particularly\npowerful if facing a large dataset, as the first layer can use a fast approach\nsuch as blending, while subsequent layers fitted on the remaining data can\nuse more computationally intensive approaches. The type of layer, along with\nany parameter settings pertaining to that layer, are specified in the\n``add`` method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mlens.ensemble import SequentialEnsemble\n\nensemble = SequentialEnsemble()\n\n# The initial layer is a blended layer, same as a layer in the BlendEnsemble\nensemble.add('blend',\n             [SVC(), RandomForestClassifier(random_state=seed)])\n\n# The second layer is a stacked layer, same as a layer of the SuperLearner\nensemble.add('stack', [SVC(), RandomForestClassifier(random_state=seed)])\n\n# The third layer is a subsembled layer, same as a layer of the Subsemble\nensemble.add('subsemble', [SVC(), RandomForestClassifier(random_state=seed)])\n\n# The meta estimator is added as in any other ensemble\nensemble.add_meta(SVC())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The below table maps the types of layers available in the :class:`SequentialEnsemble` with the corresponding ensemble.\n\n===================  ============================\nfront-end parameter  SequentialEnsemble parameter\n===================  ============================\n'SuperLearner'       'stack'\n'BlendEnsemble'      'blend'\n'Subsemble'          'subsemble'\n===================  ============================\n\nOnce instantiated, the :class:`SequentialEnsemble`` behaves as expect:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "preds = ensemble.fit(X[:75], y[:75]).predict(X[75:])\naccuracy_score(preds, y[75:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this case, the multi-layer :class:`SequentialEnsemble` with an initial\nblended layer and second stacked layer achieves similar performance as the\n:class:`BlendEnsemble` with probabilistic learning. Note that we could have\nmade any of the layers probabilistic by setting ``Proba=True``.\n\n\nPassing file paths as data input\n--------------------------------\n\nWith large datasets, it can be expensive to load the full data into memory as\na numpy array. Since ML-Ensemle uses a memmaped cache, the need to keep the\nfull array in memory can be entirely circumvented by passing a file path as\nentry to ``X`` and ``y``. There are two important things to note when doing\nthis.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, ML-Ensemble delpoys Scikit-learn's array checks, and passing a\nstring will cause an error. To avoid this, the ensemble must be initialized\nwith ``array_check=0``, in which case there will be no checks on the array.\nThe user should make certain that the the data is approprate for esitmation,\nby converting missing values and infinites to numerical representation,\nensuring that all features are numerical, and remove any headers,\nindex columns and footers.\n\nSecond, ML-Ensemble expects the file to be either a ``csv``,\nan ``npy`` or ``mmap`` file and will treat these differently.\n\n    - If a path to a ``csv`` file is passed, the ensemble will first **load**\n      the file into memory, then dump it into the cache, before discarding the\n      file from memory by replacing it with a pointer to the memmaped file.\n      The loading module used for the ``csv``\n      file is the :func:`numpy.loadtxt` function.\n\n    - If a path to a ``npy`` file is passed, a memmaped pointer to it will be\n      loaded.\n\n    - If a path to a ``mmap`` file is passed, it will be used as the memmaped\n      input array for estimation.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport tempfile\n\n# We create a temporary folder in the current working directory\ntemp = tempfile.TemporaryDirectory(dir=os.getcwd())\n\n# Dump the X and y array in the temporary directory, here as csv files\nfx = os.path.join(temp.name, 'X.csv')\nfy = os.path.join(temp.name, 'y.csv')\n\nnp.savetxt(fx, X)\nnp.savetxt(fy, y)\n\n# We can now fit any ensemble simply by passing the file pointers ``fx`` and\n# ``fy``. Remember to set ``array_check=0``.\nensemble = build_ensemble(False, array_check=0)\nensemble.fit(fx, fy)\npreds = ensemble.predict(fx)\nprint(preds[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you are following the examples on your machine,\ndon't forget to remove the temporary directory.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    temp.cleanup()\n    del temp\nexcept OSError:\n    # This can fail on Windows\n    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nEnsemble model selection\n------------------------\n\nEnsembles benefit from a diversity of base learners, but often it is not clear\nhow to parametrize the base learners. In fact, combining base learners with\nlower predictive power can often yield a superior ensemble. This hinges on the\nerrors made by the base learners being relatively uncorrelated, thus allowing\na meta estimator to learn how to overcome each model's weakness. But with\nhighly correlated errors, there is little for the ensemble to learn from.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To fully exploit the learning capacity in an ensemble, it is beneficial to\nconduct careful hyper parameter tuning, treating the base learner's parameters\nas the parameters of the ensemble. By far the most critical part of the\nensemble is the meta learner, but selecting an appropriate meta learner can be\nan ardous task if the entire ensemble has to be evaluated each time.\n\n.. py:currentmodule:: mlens.preprocessing\n\nYou can either use the ensemble as-is as a preprocessing pipeline. But do note\nthat the ``transform`` method on an ensemble used the prediction strategy from\nthe ``fit`` call, so this will not be entirely consistent with the ensemble's\nbehavior on a test set.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Alternatively, to get an exact emulator of how the ensemble would behave, use\nthe dedicated :class:`EnsembleTransformer` class, which faithfully replicated\nthe ensemble's behavior on both training and test folds.\n\n.. py:currentmodule:: mlens.ensemble\n\nThe transformer follows the same API as the :class:`SequentialEnsemble`, but\ndoes not implement a meta estimator and the transform method will\ndetermine whether the passed input data is the same as during training,\nin which case it recovers the predictions from the ``fit`` call. In the following example,\nwe run model selection on the meta learner of a blend ensemble, and try\ntwo configurations of the blend ensemble: learning from class predictions or\nfrom probability distributions over classes.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mlens.model_selection import Evaluator, EnsembleTransformer\n\nfrom mlens.metrics import make_scorer\nfrom scipy.stats import uniform, randint\n\n# Set up two competing ensemble bases as preprocessing transformers:\n# one stacked ensemble base with proba and one without\nbase_learners = [RandomForestClassifier(random_state=seed),\n                 SVC(probability=True)]\n\nproba_transformer = EnsembleTransformer(random_state=seed).add('blend', base_learners, proba=True)\nclass_transformer = EnsembleTransformer(random_state=seed).add('blend', base_learners, proba=False)\n\n# Set up a preprocessing mapping\n# Each pipeline in this map is fitted once on each fold before\n# evaluating candidate meta learners.\npreprocessing = {'proba': [('layer-1', proba_transformer)],\n                 'class': [('layer-1', class_transformer)]}\n\n# Set up candidate meta learners\n# We can specify a dictionary if we wish to try different candidates on\n# different cases, or a list if all estimators should be run on all\n# preprocessing pipelines (as in this example)\nmeta_learners = [SVC(), ('rf', RandomForestClassifier(random_state=seed))]\n\n# Set parameter mapping\n# Here, we differentiate distributions between cases for the random forest\nparams = {'svc': {'C': uniform(0, 10)},\n          'class.rf': {'max_depth': randint(2, 10)},\n          'proba.rf': {'max_depth': randint(2, 10),\n                            'max_features': uniform(0.5, 0.5)}\n          }\n\nscorer = make_scorer(accuracy_score)\nevaluator = Evaluator(scorer=scorer, random_state=seed, cv=2)\n\nevaluator.fit(X, y, meta_learners, params, preprocessing=preprocessing, n_iter=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now compare the performance of the best fit for each candidate\nmeta learner.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Results:\\n%s\" % evaluator.results)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}